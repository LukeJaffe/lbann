<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>LBANN: src/models/lbann_model_autoencoder_stacked.cpp Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.6.1 -->
<h1>src/models/lbann_model_autoencoder_stacked.cpp</h1><a href="lbann__model__autoencoder__stacked_8cpp.html">Go to the documentation of this file.</a><div class="fragment"><pre class="fragment"><a name="l00001"></a>00001 
<a name="l00002"></a>00002 <span class="comment">// Copyright (c) 2014-2016, Lawrence Livermore National Security, LLC. </span>
<a name="l00003"></a>00003 <span class="comment">// Produced at the Lawrence Livermore National Laboratory. </span>
<a name="l00004"></a>00004 <span class="comment">// Written by the LBANN Research Team (B. Van Essen, et al.) listed in</span>
<a name="l00005"></a>00005 <span class="comment">// the CONTRIBUTORS file. &lt;lbann-dev@llnl.gov&gt;</span>
<a name="l00006"></a>00006 <span class="comment">//</span>
<a name="l00007"></a>00007 <span class="comment">// LLNL-CODE-697807.</span>
<a name="l00008"></a>00008 <span class="comment">// All rights reserved.</span>
<a name="l00009"></a>00009 <span class="comment">//</span>
<a name="l00010"></a>00010 <span class="comment">// This file is part of LBANN: Livermore Big Artificial Neural Network</span>
<a name="l00011"></a>00011 <span class="comment">// Toolkit. For details, see http://software.llnl.gov/LBANN or</span>
<a name="l00012"></a>00012 <span class="comment">// https://github.com/LLNL/LBANN. </span>
<a name="l00013"></a>00013 <span class="comment">//</span>
<a name="l00014"></a>00014 <span class="comment">// Licensed under the Apache License, Version 2.0 (the &quot;Licensee&quot;); you</span>
<a name="l00015"></a>00015 <span class="comment">// may not use this file except in compliance with the License.  You may</span>
<a name="l00016"></a>00016 <span class="comment">// obtain a copy of the License at:</span>
<a name="l00017"></a>00017 <span class="comment">//</span>
<a name="l00018"></a>00018 <span class="comment">// http://www.apache.org/licenses/LICENSE-2.0</span>
<a name="l00019"></a>00019 <span class="comment">//</span>
<a name="l00020"></a>00020 <span class="comment">// Unless required by applicable law or agreed to in writing, software</span>
<a name="l00021"></a>00021 <span class="comment">// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<a name="l00022"></a>00022 <span class="comment">// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or</span>
<a name="l00023"></a>00023 <span class="comment">// implied. See the License for the specific language governing</span>
<a name="l00024"></a>00024 <span class="comment">// permissions and limitations under the license.</span>
<a name="l00025"></a>00025 <span class="comment">//</span>
<a name="l00026"></a>00026 <span class="comment">// lbann_model_stacked_autoencoder .hpp .cpp - Stacked autoencoder for</span>
<a name="l00027"></a>00027 <span class="comment">//                                             sequential neural network models</span>
<a name="l00029"></a>00029 <span class="comment"></span>
<a name="l00030"></a>00030 <span class="preprocessor">#include &quot;<a class="code" href="lbann__model__autoencoder__stacked_8hpp.html">lbann/models/lbann_model_autoencoder_stacked.hpp</a>&quot;</span>
<a name="l00031"></a>00031 <span class="preprocessor">#include &lt;string&gt;</span>
<a name="l00032"></a>00032 
<a name="l00033"></a>00033 <span class="keyword">using namespace </span>std;
<a name="l00034"></a>00034 <span class="keyword">using namespace </span>El;
<a name="l00035"></a>00035 
<a name="l00036"></a>00036 
<a name="l00038"></a>00038 <span class="comment">// stacked_autoencoder : main auto encoder class</span>
<a name="l00040"></a>00040 <span class="comment"></span>
<a name="l00041"></a><a class="code" href="classlbann_1_1StackedAutoencoder.html#a3de0e9de593444176c44ed0908fa98ad">00041</a> <a class="code" href="classlbann_1_1StackedAutoencoder.html#a3de0e9de593444176c44ed0908fa98ad">lbann::StackedAutoencoder::StackedAutoencoder</a>(<a class="code" href="classlbann_1_1Optimizer__factory.html">Optimizer_factory</a> *optimizer_factory, <span class="keyword">const</span> <a class="code" href="datatype_8hpp.html#a91ad9478d81a7aaf2593e8d9c3d06a14">uint</a> MiniBatchSize, <a class="code" href="classlbann_1_1lbann__comm.html">lbann_comm</a>* <a class="code" href="lbann__file__io_8cpp.html#ab048c6f9fcbcfaa57ce68b00263dbebe">comm</a>)
<a name="l00042"></a>00042         : <a class="code" href="classlbann_1_1AutoEncoder.html">AutoEncoder</a>(optimizer_factory, MiniBatchSize, comm)
<a name="l00043"></a>00043 {
<a name="l00044"></a>00044 <span class="preprocessor">#if 0</span>
<a name="l00045"></a>00045 <span class="preprocessor"></span>        <span class="comment">// create input layer only (no hidden/out layer)</span>
<a name="l00046"></a>00046         Layers.push_back(<span class="keyword">new</span> <a class="code" href="classlbann_1_1FullyConnectedLayer.html">FullyConnectedLayer</a>(0, -1, inputNeurons, MiniBatchSize, ActivationType, DropOut, lambda, comm));
<a name="l00047"></a>00047 <span class="preprocessor">#endif</span>
<a name="l00048"></a>00048 <span class="preprocessor"></span>}
<a name="l00049"></a>00049 
<a name="l00050"></a><a class="code" href="classlbann_1_1StackedAutoencoder.html#adc3a54ad874d23ceac93f145a80709f9">00050</a> <a class="code" href="classlbann_1_1StackedAutoencoder.html#adc3a54ad874d23ceac93f145a80709f9">lbann::StackedAutoencoder::~StackedAutoencoder</a>()
<a name="l00051"></a>00051 {
<a name="l00052"></a>00052 }
<a name="l00053"></a>00053 
<a name="l00054"></a>00054 <span class="preprocessor">#if 0</span>
<a name="l00055"></a>00055 <span class="preprocessor"></span><span class="keywordtype">void</span> lbann::beginStack(<span class="keywordtype">int</span> LayerNeurons, <a class="code" href="classlbann_1_1lbann__comm.html">lbann_comm</a>* <a class="code" href="lbann__file__io_8cpp.html#ab048c6f9fcbcfaa57ce68b00263dbebe">comm</a>)
<a name="l00056"></a>00056 {
<a name="l00057"></a>00057         <span class="comment">// get prev neurons</span>
<a name="l00058"></a>00058         <span class="keywordtype">int</span> mid = (int)Layers.size() / 2;
<a name="l00059"></a>00059         <a class="code" href="classlbann_1_1FullyConnectedLayer.html">FullyConnectedLayer</a>* player = (<a class="code" href="classlbann_1_1FullyConnectedLayer.html">FullyConnectedLayer</a>*)Layers[mid];
<a name="l00060"></a>00060         <span class="keywordtype">int</span> pneurons = player-&gt;<a class="code" href="classlbann_1_1Layer.html#a9d4a379d5c9e1102e63b48c53dd8ed44">NumNeurons</a>;
<a name="l00061"></a>00061 
<a name="l00062"></a>00062         <span class="keywordflow">if</span> (Layers.size() == 1) {
<a name="l00063"></a>00063                 <span class="comment">// create first hidden layer</span>
<a name="l00064"></a>00064                 Layers.push_back(<span class="keyword">new</span> <a class="code" href="classlbann_1_1FullyConnectedLayer.html">FullyConnectedLayer</a>(1, pneurons, LayerNeurons, MiniBatchSize, ActivationType, DropOut, Lambda, comm));
<a name="l00065"></a>00065                 <span class="comment">// create output layer</span>
<a name="l00066"></a>00066                 Layers.push_back(<span class="keyword">new</span> <a class="code" href="classlbann_1_1FullyConnectedLayer.html">FullyConnectedLayer</a>(2, LayerNeurons, pneurons, MiniBatchSize,
<a name="l00067"></a>00067                                                                                                  ActivationType, DropOut, Lambda, comm));
<a name="l00068"></a>00068         }
<a name="l00069"></a>00069         <span class="keywordflow">else</span> {
<a name="l00070"></a>00070                 <span class="comment">// create hiden layer</span>
<a name="l00071"></a>00071                 Layers.insert(Layers.begin() + mid + 1,
<a name="l00072"></a>00072                                           <span class="keyword">new</span> FullyConnectedLayer(-1, pneurons, LayerNeurons, MiniBatchSize,
<a name="l00073"></a>00073                                                                                           ActivationType, DropOut, Lambda, comm));
<a name="l00074"></a>00074 
<a name="l00075"></a>00075                 <span class="comment">// create mirror layer</span>
<a name="l00076"></a>00076                 Layers.insert(Layers.begin() + mid + 2,
<a name="l00077"></a>00077                                           <span class="keyword">new</span> FullyConnectedLayer(-1, LayerNeurons, pneurons, MiniBatchSize,
<a name="l00078"></a>00078                                                                                           ActivationType, DropOut, Lambda, comm));
<a name="l00079"></a>00079 
<a name="l00080"></a>00080                 <span class="comment">// re-number layer index</span>
<a name="l00081"></a>00081                 <span class="keywordflow">for</span> (<span class="keywordtype">int</span> n = 0; n &lt; (int)Layers.size(); n++) {
<a name="l00082"></a>00082                         Layer* layer = Layers[n];
<a name="l00083"></a>00083                         layer-&gt;Index = n;
<a name="l00084"></a>00084                 }
<a name="l00085"></a>00085         }
<a name="l00086"></a>00086 
<a name="l00088"></a>00088         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> n = 0; n &lt; (int)Layers.size(); n++) {
<a name="l00089"></a>00089                 FullyConnectedLayer* layer = (FullyConnectedLayer*)Layers[n];
<a name="l00090"></a>00090                 printf(<span class="stringliteral">&quot;[%d] layer dim: %d\n&quot;</span>, n, layer-&gt;NumNeurons);
<a name="l00091"></a>00091 
<a name="l00092"></a>00092         }
<a name="l00093"></a>00093 }
<a name="l00094"></a>00094 
<a name="l00095"></a>00095 <span class="keywordtype">void</span> lbann::trainStack(<a class="code" href="lbann__base_8hpp.html#aaf89a79f1476644edba844c4937abbcc">CircMat</a>&amp; X, <span class="keyword">const</span> <span class="keywordtype">float</span> LearnRate, <span class="keyword">const</span> <span class="keywordtype">int</span> LearnRateMethod, <span class="keyword">const</span> <a class="code" href="lbann__base_8hpp.html#a279b64f47fb2213ad73e59be937afcfa">DataType</a> DecayRate)
<a name="l00096"></a>00096 {
<a name="l00097"></a>00097         <span class="comment">// setup input (last/additional row should always be 1)</span>
<a name="l00098"></a>00098         Copy(X, Layers[0]-&gt;Acts);
<a name="l00099"></a>00099 
<a name="l00100"></a>00100         <span class="keywordtype">int</span> l1 = (int)Layers.size() / 2;
<a name="l00101"></a>00101         <span class="keywordtype">int</span> l0 = l1 - 1;
<a name="l00102"></a>00102         <span class="keywordtype">int</span> l2 = l1 + 1;
<a name="l00103"></a>00103 
<a name="l00104"></a>00104         <span class="comment">// forward propagation (until current hidden layer + 1)</span>
<a name="l00105"></a>00105   <a class="code" href="lbann__base_8hpp.html#a279b64f47fb2213ad73e59be937afcfa">DataType</a> L2NormSum = 0;
<a name="l00106"></a>00106         <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> l = 1; l &lt;= l2; l++) {
<a name="l00107"></a>00107                 L2NormSum = Layers[l]-&gt;forwardProp(L2NormSum);
<a name="l00108"></a>00108         }
<a name="l00109"></a>00109 
<a name="l00110"></a>00110         <span class="comment">// backward propagation (from current hidden layer + 1)</span>
<a name="l00111"></a>00111         Layers[l2]-&gt;backProp(*Layers[l1], Layers[l0]-&gt;Acts);
<a name="l00112"></a>00112         Layers[l1]-&gt;backProp(*Layers[l0], *Layers[l2]);
<a name="l00113"></a>00113 
<a name="l00114"></a>00114         <span class="comment">// update weights, biases</span>
<a name="l00115"></a>00115         Layers[l1]-&gt;updateMB(LearnRate, LearnRateMethod, DecayRate);
<a name="l00116"></a>00116         Layers[l2]-&gt;updateMB(LearnRate, LearnRateMethod, DecayRate);
<a name="l00117"></a>00117 }
<a name="l00118"></a>00118 
<a name="l00119"></a>00119 <span class="keywordtype">void</span> lbann::endStack()
<a name="l00120"></a>00120 {
<a name="l00121"></a>00121 
<a name="l00122"></a>00122 }
<a name="l00123"></a>00123 
<a name="l00124"></a>00124 <span class="preprocessor">#endif</span>
</pre></div></div>
<hr size="1"/><address style="text-align: right;"><small>Generated on 21 Sep 2016 for LBANN by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.6.1 </small></address>
</body>
</html>
