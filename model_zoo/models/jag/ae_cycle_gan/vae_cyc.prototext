#Example taken from: https://lc.llnl.gov/bitbucket/users/jjayaram/repos/deep-latent-spaces/browse/codes/dev/VAE-FCN/vae_fcn.py and
#https://lc.llnl.gov/bitbucket/users/jjayaram/repos/deep-latent-spaces/browse/codes/dev/VAE-FCN/run_vae.py
#Timestamp 02/26/2018 8:45AM
model {
  name: "directed_acyclic_graph_model"
  data_layout: "model_parallel"
  #mini_batch_size: 128
  mini_batch_size: 100 #more last minibatch images to save
  block_size: 256
  num_epochs: 40
  num_parallel_readers: 0
  procs_per_model: 0

  ###################################################
  # Objective function
  ###################################################

  objective_function {
    binary_cross_entropy {}
    #mean_squared_error {}
    layer_term {
      scale_factor: 1.0
      layer: "kl_divergence"
    }
    l2_weight_regularization {
      scale_factor: 1e-4
    }
  }

  ###################################################
  # Metrics
  ###################################################

  metric { mean_squared_error {} }

  ###################################################
  # Callbacks
  ###################################################
  callback {
    print {
      interval: 1
    }
  }
  callback { timer {} }
 # callback {
 #   save_images {
 #     image_dir: "vae_fcn_images_"
 #     extension: "jpg"
 #   }
 # }

  ###################################################
  # start of layers
  ###################################################

  ######################
  # Data
  ######################
  #Layer from cycle GAN
  layer {
    input {
      io_buffer: "partitioned"
      target_mode: "N/A"
    }
    name: "data"
    data_layout: "data_parallel"
    parents: " "
  }
  layer {
    name: "slice_data"
    data_layout: "model_parallel"
    parents: "data"
    children: "image_data_id param_data_id"
    slice {
      slice_points: "0 16384 16389"
    }
  }
  layer {
    identity {
    }
    name: "image_data_id"
    data_layout: "model_parallel"
    parents: "slice_data"
  }
  layer {
    identity {
    }
    name: "param_data_id"
    data_layout: "model_parallel"
    parents: "slice_data"
  }
  layer {
    fully_connected {
      num_neurons: 256
      has_bias: true
    }
    name: "gen1fc1_1"
    data_layout: "model_parallel"
    weights: "gen1fc1linearity"
    parents: "param_data_id"
  }
  layer {
    relu {
    }
    name: "gen1relu1_1"
    data_layout: "model_parallel"
    parents: "gen1fc1_1"
  }
  layer {
    fully_connected {
      num_neurons: 2048
      has_bias: true
    }
    name: "gen1fc2_1"
    data_layout: "model_parallel"
    weights: "gen1fc2linearity"
    parents: "gen1relu1_1"
  }
  layer {
    relu {
    }
    name: "gen1relu2_1"
    data_layout: "model_parallel"
    parents: "gen1fc2_1"
  }
  layer {
    dropout {
      keep_prob: 0.8
    }
    name: "gen1dropout1_1"
    data_layout: "model_parallel"
    parents: "gen1relu2_1"
  }
  layer {
    fully_connected {
      num_neurons: 8192
      has_bias: true
    }
    name: "gen1fc3_1"
    data_layout: "model_parallel"
    weights: "gen1fc3linearity"
    parents: "gen1dropout1_1"
  }
  layer {
    relu {
    }
    name: "gen1relu3_1"
    data_layout: "model_parallel"
    parents: "gen1fc3_1"
  }
  layer {
    fully_connected {
      #num_neurons: 16384
      #latent_dim
      num_neurons: 20
      has_bias: true
    }
    name: "gen1fc4_1"
    data_layout: "model_parallel"
    weights: "gen1fc4linearity"
    parents: "gen1relu3_1"
  }
  
  weights {
    name: "gen1fc1linearity"
    he_normal_initializer {
    }
  }
  weights {
    name: "gen1fc2linearity"
    he_normal_initializer {
    }
  }
  weights {
    name: "gen1fc3linearity"
    he_normal_initializer {
    }
  }
  weights {
    name: "gen1fc4linearity"
    he_normal_initializer {
    }
  }

  ######################
  # Encoder
  ######################
  #Encoder not really used here
  # encode1
  layer {
    parents: "image_data_id"
    name: "encode1"
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 256
      weight_initialization: "he_normal"
      has_bias: true
    }
  }
  layer {
    parents: "encode1"
    name: "encode1_elu"
    data_layout: "model_parallel"
    device_allocation: "cpu"
    elu {}
  }
  layer {
    parents: "encode1_elu"
    name: "encode1_dropout"
    data_layout: "model_parallel"
    dropout {
      keep_prob: 0.95
    }
  }

  # encode2
  layer {
    parents: "encode1_dropout"
    name: "encode2"
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 256
      weight_initialization: "he_normal"
      has_bias: true
    }
  }
  layer {
    parents: "encode2"
    name: "encode2_tanh"
    data_layout: "model_parallel"
    tanh {}
  }
  layer {
    parents: "encode2_tanh"
    name: "encode2_dropout"
    data_layout: "model_parallel"
    dropout {
      keep_prob: 0.95
    }
  }

  # encode3
  layer {
    parents: "encode2_dropout"
    name: "encode3"
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 256
      weight_initialization: "he_normal"
      has_bias: true
    }
  }
  layer {
    parents: "encode3"
    name: "encode3_tanh"
    data_layout: "model_parallel"
    tanh {}
  }
  layer {
    parents: "encode3_tanh"
    name: "encode3_dropout"
    data_layout: "model_parallel"
    dropout {
      keep_prob: 0.95
    }
  }

  ######################
  # Latent space
  ######################

  layer {
    parents: "encode3_dropout"
    name: "z_mean"
    data_layout: "model_parallel"
    fully_connected {
      num_neurons:20
      weight_initialization: "glorot_normal"
      has_bias: true
    }
  }
  layer {
    parents: "encode3_dropout"
    name: "z_log_sigma"
    data_layout: "model_parallel"
    fully_connected {
      num_neurons:20
      weight_initialization: "glorot_normal"
      has_bias: true
    }
  }

  ######################
  # KL divergence
  ######################

  layer {
    name: "kl_one"
    data_layout: "model_parallel"
    constant {
      value: 1.0
      num_neurons: "20"
    }
  }
  layer {
    parents: "z_mean"
    name: "kl_z_mean2"
    data_layout: "model_parallel"
    device_allocation: "cpu"
    square {}
  }
  layer {
    parents: "z_log_sigma"
    name: "kl_exp"
    data_layout: "model_parallel"
    device_allocation: "cpu"
    exp {}
  }
  layer {
    parents: "kl_one z_log_sigma kl_z_mean2 kl_exp"
    name: "kl_full"
    data_layout: "model_parallel"
    weighted_sum {
      scaling_factors: "-0.5 -0.5 0.5 0.5"
    }
  }
  layer {
    parents: "kl_full"
    name: "kl_sum"
    data_layout: "data_parallel"
    reduction {
      mode: "sum"
    }
  }
  layer {
    parents: "kl_sum"
    name: "kl_divergence"
    data_layout: "data_parallel"
    evaluation {}
  }

  ######################
  # Sample from latent space
  ######################

  layer {
    parents: "z_log_sigma"
    name: "sample_half"
    data_layout: "model_parallel"
    weighted_sum {
      scaling_factors: "0.5"
    }
  }
  layer {
    parents: "sample_half"
    name: "sample_exp"
    data_layout: "model_parallel"
    device_allocation: "cpu"
    exp {}
  }
  layer {
    name: "sample_noise"
    data_layout: "model_parallel"
    gaussian {
      mean: 0.0
      stdev: 1.0
      neuron_dims: "20"
    }
  }
  layer {
    parents: "sample_exp sample_noise"
    name: "sample_exp_noise"
    data_layout: "model_parallel"
    hadamard {}
  }
  layer {
    parents: "z_mean sample_exp_noise"
    #name: "sample"
    name: "image_data_dummy"
    data_layout: "model_parallel"
    sum {}
  }
  ####output of encoder not used, dangling
  ######################
  # Decoder
  ######################

  # decode3
  layer {
    #parents: "sample"
    parents: "gen1fc4_1"
    name: "decode3"
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 256
      weight_initialization: "he_normal"
      has_bias: true
    }
  }
  layer {
    parents: "decode3"
    name: "decode3_tanh"
    data_layout: "model_parallel"
    tanh {}
  }
  layer {
    parents: "decode3_tanh"
    name: "decode3_dropout"
    data_layout: "model_parallel"
    dropout {
      keep_prob: 0.95
    }
  }

  # decode2
  layer {
    parents: "decode3_dropout"
    name: "decode2"
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 256
      weight_initialization: "he_normal"
      has_bias: true
    }
  }
  layer {
    parents: "decode2"
    name: "decode2_tanh"
    data_layout: "model_parallel"
    tanh {}
  }
  layer {
    parents: "decode2_tanh"
    name: "decode2_dropout"
    data_layout: "model_parallel"
    dropout {
      keep_prob: 0.95
    }
  }

  # decode1
  layer {
    parents: "decode2_dropout"
    name: "decode1"
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 256
      weight_initialization: "he_normal"
      has_bias: true
    }
  }
  layer {
    parents: "decode1"
    name: "decode1_elu"
    data_layout: "model_parallel"
    device_allocation: "cpu"
    elu {
    }
  }
  layer {
    parents: "decode1_elu"
    name: "decode1_dropout"
    data_layout: "model_parallel"
    dropout {
      keep_prob: 0.95
    }
  }

  # decode0
  layer {
    parents: "decode1_dropout"
    name: "decode0"
    data_layout: "model_parallel"
    #num_neurons_from_data_reader: true
    fully_connected {
      weight_initialization: "glorot_normal"
      num_neurons: 16384
      has_bias: true
    }
  }
  layer {
    parents: "decode0"
    name: "sigmoid"
    data_layout: "model_parallel"
    sigmoid {}
  }

  ######################
  # Reconstruction
  ######################

  layer {
    #parents: "sigmoid data"
    #parents: "sigmoid gen1fc4_1"
    parents: "sigmoid image_data_id"
    name: "reconstruction"
    data_layout: "model_parallel"
    reconstruction {}
  }

  ###################################################
  # end of layers
  ###################################################
}
