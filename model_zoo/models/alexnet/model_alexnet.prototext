model {
  name: "dnn"
  objective_function: "cross_entropy"
  metric {
    categorical_accuracy {
    }
  }
  metric {
    top_k_categorical_accuracy {
       top_k: 5
    }
  }
  data_layout: "data_parallel"
  mini_batch_size: 256
  block_size: 256
  num_epochs: 20
  num_parallel_readers: 0
  procs_per_model: 0
  use_cudnn: true
  num_gpus: -1

  ###################################################
  # Callbacks 
  ###################################################
  callback {
    imcomm {
      intermodel_comm_method: "normal"
      layers: "2 6 10 12 14 17 20 23"
    }  
  }
  callback {
    print {
      interval: 1
    }
  }
  callback {
    timer {
    }
  }
  callback {
    summary {
      dir: "."
      batch_interval: 1
      mat_interval: 25
    }
  }
  # callback {
  #   save_images {
  #     image_dir: ""
  #     extension: "jpg"
  #   }
  # }
  # callback {
  #   dump_mb_indices {
  #     basename: "debug_alexnet/"
  #     interval: 1
  #   }
  # }
  # callback {
  #   disp_io_stats {
  #     layers: "1"
  #   }
  # }
  # callback {
  #   debug {
  #     phase: "train"
  #   }
  # }
  # callback {
  #   gradient_check {
  #     step_size: 0
  #     verbose: false
  #   }
  # }

  ###################################################
  # start of layers
  ###################################################

  #######
  # INPUT
  #######
  layer {
    index: 1
    parent: 1
    data_layout: "data_parallel"
    input_partitioned_minibatch {
    }
  }

  #############
  # CONVOLUTION 1
  #############
  layer {
    index: 2
    parent: 1
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels: 96
      conv_dims: "11 11"
      conv_pads: "0 0"
      conv_strides: "4 4"
      weight_initialization: "he_normal"
      has_bias: true
      has_vectors: true
      l2_regularization_factor: 0.0005
    }
  }

  ######
  # RELU
  ######
  layer {
    index: 3
    parent: 2
    data_layout: "data_parallel"
    relu {
    }
  }

  #############
  # LRN 2
  #############
  layer {
    index: 4
    parent: 3
    data_layout: "data_parallel"
    local_response_normalization {
      window_width: 5
      lrn_alpha: 0.0001
      lrn_beta: 0.75
      lrn_k: 2
    }
  }

  #########
  # POOLING 3
  #########
  layer {
    index: 5
    parent: 4
    data_layout: "data_parallel"
    pooling {
      num_dims: 2
      pool_dims: "3 3"
      pool_pads: "0 0"
      pool_strides: "2 2"
      pool_mode: "max"
      has_vectors: true
    }
  }

  #############
  # CONVOLUTION 4
  #############
  layer {
    index: 6
    parent: 5
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels:  256
      conv_dims: "5 5"
      conv_pads: "2 2"
      conv_strides: "1 1"
      weight_initialization: "he_normal"
      has_bias: true
      bias_initial_value: 0.1
      has_vectors: true
      l2_regularization_factor: 0.0005
    }
  }

  ######
  # RELU
  ######
  layer {
    index: 7
    parent: 6
    data_layout: "data_parallel"
    relu {
    }
  }

  #############
  # LRN 5
  #############
  layer {
    index: 8
    parent: 7
    data_layout: "data_parallel"
    local_response_normalization {
      window_width: 5
      lrn_alpha: 0.0001
      lrn_beta: 0.75
      lrn_k: 2
    }
  }


  #########
  # POOLING 6
  #########
  layer {
    index: 9
    parent: 8
    data_layout: "data_parallel"
    pooling {
      num_dims: 2
      pool_dims: "3 3"
      pool_pads: "0 0"
      pool_strides: "2 2"
      pool_mode: "max"
      has_vectors: true
    }
  }

  #############
  # CONVOLUTION 7
  #############
  layer {
    index: 10
    parent: 9
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels:  384
      conv_dims: "3 3"
      conv_pads: "1 1"
      conv_strides: "1 1"
      weight_initialization: "he_normal"
      has_bias: true
      has_vectors: true
      l2_regularization_factor: 0.0005
    }
  }

  ######
  # RELU
  ######
  layer {
    index: 11
    parent: 10
    data_layout: "data_parallel"
    relu {
    }
  }

  #############
  # CONVOLUTION 8
  #############
  layer {
    index: 12
    parent: 11
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels:  384
      conv_dims: "3 3"
      conv_pads: "1 1"
      conv_strides: "1 1"
      weight_initialization: "he_normal"
      has_bias: true
      bias_initial_value: 0.1
      has_vectors: true
      l2_regularization_factor: 0.0005
    }
  }

  ######
  # RELU
  ######
  layer {
    index: 13
    parent: 12
    data_layout: "data_parallel"
    relu {
    }
  }

  #############
  # CONVOLUTION 9
  #############
  layer {
    index: 14
    parent: 13
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels:  256
      conv_dims: "3 3"
      conv_pads: "1 1"
      conv_strides: "1 1"
      weight_initialization: "he_normal"
      has_bias: true
      bias_initial_value: 0.1
      has_vectors: true
      l2_regularization_factor: 0.0005
    }
  }

  ######
  # RELU
  ######
  layer {
    index: 15
    parent: 14
    data_layout: "data_parallel"
    relu {
    }
  }

  #########
  # POOLING
  #########
  layer {
    index: 16
    parent: 15
    data_layout: "data_parallel"
    pooling {
      num_dims: 2
      pool_dims: "3 3"
      pool_pads: "0 0"
      pool_strides: "2 2"
      pool_mode: "max"
      has_vectors: true
    }
  }


  #################
  # FULLY_CONNECTED 11
  #################
  layer {
    index: 17
    parent: 16
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 4096
      weight_initialization: "he_normal"
      has_bias: true
      bias_initial_value: 0.1
      l2_regularization_factor: 0.0005
    }
  }

  ######
  # RELU
  ######
  layer {
    index: 18
    parent: 17
    data_layout: "model_parallel"
    relu {
    }
  }

  #########
  # DROPOUT
  #########
  layer {
    index: 19
    parent: 18
    data_layout: "model_parallel"
    dropout {
      keep_prob: 0.5
    }
  }

  #################
  # FULLY_CONNECTED 12
  #################
  layer {
    index: 20
    parent: 19
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 4096
      weight_initialization: "he_normal"
      has_bias: true
      bias_initial_value: 0.1
      l2_regularization_factor: 0.0005
    }
  }

  ######
  # RELU
  ######
  layer {
    index: 21
    parent: 20
    data_layout: "model_parallel"
    relu {
    }
  }

  #########
  # DROPOUT
  #########
  layer {
    index: 22
    parent: 21
    data_layout: "model_parallel"
    dropout {
      keep_prob: 0.5
    }
  }

  #################
  # FULLY_CONNECTED
  #################
  layer {
    index: 23
    parent: 22
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 1000
      weight_initialization: "he_normal"
      has_bias: false
      l2_regularization_factor: 0.0005
    }
  }

  #########
  # SOFTMAX
  #########
  layer {
    index: 24
    parent: 23
    data_layout: "model_parallel"
    softmax {
    }
  }

  ########
  # TARGET
  ########
  layer {
    index: 25
    parent: 24
    data_layout: "data_parallel"
    target_partitioned_minibatch {
      shared_data_reader: true
    }
  }
}
