model {
  ### Model description and network architecture taken from:
  ### https://lc.llnl.gov/bitbucket/projects/BIOM/repos/molresp/browse/tf_model.py?at=TensorFlow_chemClass
  ### This network description is anologous to AutoEncoder_Chem_Sigmoid 
  name: "dnn"
  objective_function: "mean_squared_error"
  data_layout: "model_parallel"
  mini_batch_size: 128
  block_size: 256
  num_epochs: 4
  num_parallel_readers: 1
  procs_per_model: 0
  use_cudnn: true
  num_gpus: -1

  ###################################################
  # Callbacks
  ###################################################
  callback {
    print {
      interval: 1
    }
  }
  callback {
    timer {
    }
  }
  callback {
    summary {
      dir: "."
      batch_interval: 1
      mat_interval: 25
    }
  }
#  callback {
#    debug {
#      phase: "train"
#    }
#  }

  ###################################################
  # start of layers
  ###################################################

  #######
  # INPUT
  #######
  layer {
    name: "1"
    parents: "1"
    children: ""
    data_layout: "model_parallel"
    input_distributed_minibatch {
    }
  }

  #################
  # FULLY_CONNECTED encode1
  #################
  layer {
    name: "2"
    parents: "1"
    children: ""
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 2000
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  ######
  # SIGMOID sigmoid1
  ######
  layer {
    name: "3"
    parents: "2"
    children: ""
    data_layout: "model_parallel"
    sigmoid {
    }
  }

  #################
  # FULLY_CONNECTED encode2
  #################
  layer {
    name: "4"
    parents: "3"
    children: ""
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 500
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  #######
  # SIGMOID sigmoid2
  #######
  layer {
    name: "5"
    parents: "4"
    children: ""
    data_layout: "model_parallel"
    sigmoid {
    }
  }

  #################
  # FULLY_CONNECTED encode3
  #################
  layer {
    name: "6"
    parents: "5"
    children: ""
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 100 
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  #######
  # SIGMOID sigmoid3
  #######
  layer {
    name: "7"
    parents: "6"
    children: ""
    data_layout: "model_parallel"
    sigmoid {
    }
  }


  #################
  # FULLY_CONNECTED decode3
  #################
  layer {
    name: "8"
    parents: "7"
    children: ""
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 500
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  #######
  # SIGMOID sigmoid4
  #######
  layer {
    name: "9"
    parents: "8"
    children: ""
    data_layout: "model_parallel"
    sigmoid {
    }
  }

  #################
  # FULLY_CONNECTED decode2
  #################
  layer {
    name: "10"
    parents: "9"
    children: ""
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 2000
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }


  #######
  # SIGMOID sigmoid5
  #######
  layer {
    name: "11"
    parents: "10"
    children: ""
    data_layout: "model_parallel"
    sigmoid {
    }
  }

  #################
  # FULLY_CONNECTED decode1
  #################
  layer {
    name: "12"
    parents: "11"
    children: ""
    data_layout: "model_parallel"
    num_neurons_from_data_reader: true
    fully_connected {
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  #######
  # SIGMOID sigmoid6
  #######
  layer {
    name: "13"
    parents: "12"
    children: ""
    data_layout: "model_parallel"
    sigmoid {
    }
  }

  
  #################
  # RECONSTRUCTION
  #################
  layer {
    name: "14"
    parents: "13"
    children: ""
    data_layout: "model_parallel"
    reconstruction {
      original_layer: 1
    }
  }

  ###################################################
  # end of layers
  ###################################################
}
