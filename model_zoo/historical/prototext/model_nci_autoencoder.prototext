model {
  name: "dnn"
  objective_function: "mean_squared_error"
  data_layout: "model_parallel"
  mini_batch_size: 128
  block_size: 256
  num_epochs: 4
  num_parallel_readers: 1
  procs_per_model: 0
  use_cudnn: true
  num_gpus: -1

  ###################################################
  # Callbacks
  ###################################################
  callback {
    print {
      interval: 1
    }
  }
  callback {
    timer {
      dir: "none"
    }
  }
  callback {
    summary {
      dir: " "
      interval: 1
    }
  }
#  callback {
#    debug {
#      phase: "train"
#    }
#  }

  ###################################################
  # start of layers
  ###################################################

  #######
  # INPUT
  #######
  layer {
    index: 1
    parent: 1
    data_layout: "model_parallel"
    input_distributed_minibatch {
    }
  }

  #################
  # FULLY_CONNECTED encode1
  #################
  layer {
    index: 2
    parent: 1
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 1000
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  ######
  # RELU relu1
  ######
  layer {
    index: 3
    parent: 2
    data_layout: "model_parallel"
    relu {
    }
  }

  #################
  # FULLY_CONNECTED encode2
  #################
  layer {
    index: 5
    parent: 3
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 500
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  #######
  # RELU relu2
  #######
  layer {
    index: 6
    parent: 5
    data_layout: "model_parallel"
    relu {
    }
  }

  #################
  # FULLY_CONNECTED encode3
  #################
  layer {
    index: 8
    parent: 6
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 300 
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  #######
  # RELU relu3
  #######
  layer {
    index: 9
    parent: 8
    data_layout: "model_parallel"
    relu {
    }
  }

  #################
  # FULLY_CONNECTED encode4
  #################
  layer {
    index: 10
    parent: 9
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 100
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  #################
  # FULLY_CONNECTED decode4
  #################
  layer {
    index: 11
    parent: 9
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 300
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  #######
  # RELU 4
  #######
  layer {
    index: 12
    parent: 11
    data_layout: "model_parallel"
    relu {
    }
  }

  #################
  # FULLY_CONNECTED decode3
  #################
  layer {
    index: 13
    parent: 12
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 500
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }


  #######
  # RELU relu5
  #######
  layer {
    index: 14
    parent: 13
    data_layout: "model_parallel"
    relu {
    }
  }

  #################
  # FULLY_CONNECTED decode2
  #################
  layer {
    index: 15
    parent: 14
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 1000
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  #######
  # RELU relu6
  #######
  layer {
    index: 16
    parent: 15
    data_layout: "model_parallel"
    relu {
    }
  }

  #################
  # FULLY_CONNECTED decode1
  #################
  layer {
    index: 17
    parent: 16
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 1444 
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
  }

  #######
  # RELU relu7
  #######
  layer {
    index: 18
    parent: 16
    data_layout: "model_parallel"
    relu {
    }
  }

  
  #################
  # RECONSTRUCTION
  #################
  layer {
    index: 19
    parent: 18
    data_layout: "model_parallel"
    reconstruction {
      original_layer: 1
    }
  }

  ###################################################
  # end of layers
  ###################################################
}
