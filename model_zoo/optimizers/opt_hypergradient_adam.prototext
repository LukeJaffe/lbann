optimizer {
  hypergradient_adam {
    init_learning_rate: 0.001
    hyper_learning_rate: 1e-7
    beta1: 0.9
    beta2: 0.99
    eps: 1e-8
  }  
}
